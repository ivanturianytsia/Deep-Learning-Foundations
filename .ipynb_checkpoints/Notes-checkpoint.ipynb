{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning week 1 notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Books to read\n",
    "- [Deep Learning](http://www.deeplearningbook.org/)\n",
    "- [Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com/)\n",
    "- [Grokking Deep Learning](https://www.manning.com/books/grokking-deep-learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python lib tutorials\n",
    "- [Pandas](http://pandas.pydata.org/pandas-docs/stable/10min.html#min)\n",
    "- [SciKit-learn](http://scikit-learn.org/stable/tutorial/basic/tutorial.html)\n",
    "- [Pyplot](http://matplotlib.org/users/pyplot_tutorial.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression\n",
    "**Linear regression** is a method of finding best fitting values *m* and *b* for a line (*y = mx + b*), which passes through the points of a given array with a minimal mean error. Linear regression uses **gradient descent** to closesly approach the local minimum, by computing a partial derivative for each *m* and *b* along the way. As the line approaches the minimal error, derivatives decrease as well as iteration step:\n",
    "```\n",
    "new_b = b_current - (learningRate * b_gradient)\n",
    "new_m = m_current - (learningRate * m_gradient)\n",
    "```\n",
    "\n",
    "The learning rate and number of iterations is chosen manualy. Wrong learning rate, too big or too small, decreases the speed of reaching the vertex. If the learning rate is too small, low derivative will slow the descent down drastically, so the values may not reach the lower point at all. On the other hand, if the learning rate is too big, iterations will reach some level in the lower part quickly, by jumping from slope to slope in zig-zags, but they might not precisely reach the bottom of the curve in any reasonable time/number of iterations.\n",
    "\n",
    "[Code](https://github.com/llSourcell/linear_regression_live) from Siraj [live video](https://www.youtube.com/watch?v=uwwWVAgJBcM)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural networks\n",
    "\n",
    "**Neural network** - a network of neurons, which convert input data to output data. \n",
    "A **neuron** or **perceptrons** resemble a body, dendrites (inputs) and an axon (output). As signals enter the neuron through inputs, they are multiplied by **weights** - measure of how much each signal means to the output signal. Then, weighted signals are summed. The resulting signal is feeded through the **activation function**, usually a **sigmoid**, in order to normalize the output.\n",
    "```\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "output = sigmoid(dot(weights, inputs) + bias)\n",
    "```\n",
    "\n",
    "### Learning\n",
    "Learning of the neural network is done by adjusting the weights by gradient descent of the error - difference between the target output and the neural network output.\n",
    "``` \n",
    "def sigmoid_derivative(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "    \n",
    "error = target - output\n",
    "delta_weights = learnrate * error * sigmoid_derivative(dot(weights, inputs)) * input\n",
    "weights =+ delta_weights \n",
    "```\n",
    "Starting weights are chosen randomly, due to this, weight can get into a local minima. One of the method to avoid this - [momentum](http://sebastianruder.com/optimizing-gradient-descent/index.html#momentum)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supporting materials\n",
    "- [Khan academy's Gradient and directional derivatives course](https://www.khanacademy.org/math/multivariable-calculus/multivariable-derivatives/gradient-and-directional-derivatives/v/gradient)\n",
    "- [Khan academy's Chain rule course](https://www.khanacademy.org/math/multivariable-calculus/multivariable-derivatives/gradient-and-directional-derivatives/v/gradient)\n",
    "- [An overview of gradient descent optimization algorithms](http://sebastianruder.com/optimizing-gradient-descent/index.html#momentum)\n",
    "- [Khan academy's Matrices course](https://www.khanacademy.org/math/precalculus/precalc-matrices)\n",
    "- [Khan academy's Vectors course](https://www.khanacademy.org/math/linear-algebra/vectors-and-spaces/vectors/v/vector-introduction-linear-algebra)\n",
    "- [Andrej Karpathy's Yes you should understand backprop](https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b#.hzcjgsrsr)\n",
    "- [Andrej Karpathy's Backpropagation](https://www.youtube.com/watch?v=59Hbtz7XgjM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
